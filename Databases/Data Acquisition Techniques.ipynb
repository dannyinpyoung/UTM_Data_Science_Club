{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "copyrighted-application",
   "metadata": {},
   "source": [
    "# Intro to Data Acquisition\n",
    "\n",
    "## Ways to Acquire Data:\n",
    "\n",
    "### 1. Public Data\n",
    "\n",
    "Check out the following websites for access to public data. \n",
    "\n",
    "* [GitHub](https://github.com/)\n",
    "* [Kaggle](https://www.kaggle.com/)\n",
    "* [KDnuggets](https://www.kdnuggets.com/)\n",
    "* [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)\n",
    "* [US Government’s Open Data](https://www.data.gov/)\n",
    "* [Five Thirty Eight](https://data.fivethirtyeight.com/)\n",
    "* [Amazon Web Services](https://aws.amazon.com/)\n",
    "\n",
    "We can use Pandas to directly import using code. \n",
    "```Python\n",
    "# Import pandas with alias\n",
    "import pandas as pd\n",
    " \n",
    "# Assign the dataset url as a variable\n",
    "url = \"https://raw.githubusercontent.com/shrikant-temburwar/Iris-Dataset/master/Iris.csv\"\n",
    " \n",
    "# Define the column names of dataset as a list\n",
    "columns = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"]\n",
    " \n",
    "# Use read_csv to read in data as a pandas dataframe\n",
    "df = pd.read_csv(url, names=columns)\n",
    " \n",
    "# Check head of dataframe\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "### 2. Private Data\n",
    "\n",
    "There are also a number of private datasets that businesses curate themselves and are under ownership of the company. For instance, Netflix’s database of user preferences powers their immense recommendation systems. There are also services that allow you to purchase datasets such as data markets like Data & Sons or crowd-sourcing marketplaces such as Amazon’s Mechanical Turks where one can outsource their data acquisition needs like data validation and research to survey participation. Often we will find usage of private data within a large production scale setting.\n",
    "\n",
    "Pros:\n",
    "Time: Readily available datasets that can quickly move a project to the next phase of the Data Science Life Cycle.\n",
    "\n",
    "Cost: Public datasets can cut costs of collecting data down to zero.\n",
    "\n",
    "Cons:\n",
    "Messy: Data can often come in forms that require intensive cleaning and modification.\n",
    "Cost: Private services can lead to high costs in acquiring data.\n",
    "\n",
    "\n",
    "### 3. Web Scraping \n",
    "\n",
    "Most commonly used modules for webscraping are BeautifulSoup, Selenium, and Scrapy. We typically employ web scraping techniques to acquire data for small to medium sized projects, but rarely in production as this can raise ownership and copyright issues.\n",
    "\n",
    "Example Code: \n",
    "\n",
    "```Python\n",
    "# Import libraries \n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    " \n",
    "# Assign URL to variable\n",
    "url = \"https://www.codecademy.com/\"\n",
    " \n",
    "# Send request to download the data from URL\n",
    "response = requests.request(\"GET\", url)\n",
    " \n",
    "# Create BeautifulSoup object\n",
    "# Use HTML parser to parse the page's text\n",
    "data = BeautifulSoup(response.text, 'html.parser')\n",
    " \n",
    "# Print the first header of the page\n",
    "print(data.html.h1)\n",
    " \n",
    "# Instantiate list to append some content\n",
    "content = []\n",
    " \n",
    "# Use BeautifulSoup's find_all method to find all paragraph tags\n",
    "words = data.find_all('p')\n",
    " \n",
    "# Iterate through all paragraph tags\n",
    "# append text to list with for loop\n",
    "for word in words:\n",
    "    content.append(word.text)\n",
    " \n",
    "# Check content\n",
    "print(content)\n",
    " \n",
    "# Create dataframe of content with pandas DataFrame method\n",
    "df = pd.DataFrame(content, columns= ['Text'])\n",
    " \n",
    "# Check scraped dataframe\n",
    "print(df)\n",
    "```\n",
    "\n",
    "__Pros:__\n",
    "* Versatile: Highly adaptable method for acquiring data from the internet.\n",
    "* Scalable: Distributed bots can be coordinated to retrieve large quantities of data.\n",
    "\n",
    "__Cons__:\n",
    "* Language Barrier: Multiple languages are involved when scraping and require a knowledge of languages not typically used for data science.\n",
    "* Legality: Excessive or improper web-scraping can be illegal, disrupt a website’s functionality, and lead to your IP address being black listed from the site.\n",
    "\n",
    "\n",
    "### 4. APIs\n",
    "\n",
    "Unlike web scraping, APIs are a means of communication between 2 different software systems. Typically this communication is achieved in the form of an HTTP Request/Response Cycle where a client(you) sends a request to a website’s server for data through an API call. The server then searches within its databases for the particular data requested and responds back to the client either with the data, or an error stating that request can not be fulfilled.\n",
    "\n",
    "__Pros:__\n",
    "* User & Site Friendly: APIs allow security and management of resources for sites that data is being requested from.\n",
    "* Scalable: API’s can allow for various amounts of data to be requested, up to production scale volumes.\n",
    "\n",
    "__Cons:__\n",
    "* Limited: Some functions or data may not be accessed via an API.\n",
    "* Cost: Some API calls can be quite expensive, leading to limitations of certain functions and projects.\n",
    "\n",
    "# Big Data\n",
    "\n",
    "The data that these corporations maintain are so complex, they are referred to as Big Data. Data like this can not be stored on a single machine, and must often be stored in the cloud and be hosted on servers in data centers. The term Big Data is not only in reference to the sheer volume of the data, which can easily grow to the petabyte and exabyte levels, but also in the variety and velocity of the data, we often refer to these characteristics as the 3 Vs of Big Data:\n",
    "\n",
    "* __V__ariety\n",
    "* __V__olume\n",
    "* __V__elocity\n",
    "\n",
    "Google’s Chief Economist Hal Varian has listed the four key components of data acquisition in business as:\n",
    "\n",
    "* The drive toward more and more data extraction and analysis.\n",
    "* The development of new contractual forms using computer-monitoring and automation.\n",
    "* The desire to personalize and customize the services offered to users of digital platforms.\n",
    "* The use of the technological infrastructure to carry out continual experiments on its users and consumers.\n",
    "\n",
    "\n",
    "# Creating Your Own Dataset\n",
    "\n",
    "Check out [this link](https://towardsdatascience.com/how-to-build-your-own-dataset-for-data-science-projects-7f4ad0429de4) to learn about how to create your own datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-protest",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
